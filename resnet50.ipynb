{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa9599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e8efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14e41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.9), please consider upgrading to the latest version (0.3.12).\n",
      "\n",
      " Path to dataset files: C:\\Users\\vyrus\\.cache\\kagglehub\\datasets\\parsasam\\captcha-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"parsasam/captcha-dataset\")\n",
    "\n",
    "print(\"\\n Path to dataset files:\", dataset_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be508583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a6468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(img, block_size=11, C=2, invert=True):\n",
    "    # Convert to uint8\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # Adaptive threshold\n",
    "    binarized = cv2.adaptiveThreshold(\n",
    "        blurred,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        block_size,  # must be odd\n",
    "        C\n",
    "    )\n",
    "    \n",
    "    # White text on black background\n",
    "    if invert:\n",
    "        binarized = cv2.bitwise_not(binarized)\n",
    "\n",
    "    return binarized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ab58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8444f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O2ewH shape: (40, 150)\n",
      "pPDId shape: (40, 150)\n",
      "Qwlss shape: (40, 150)\n",
      "7g77Y shape: (40, 150)\n",
      "Eeh92 shape: (40, 150)\n",
      "Image shape: 40 x 150\n",
      "  filename                                              image  \\\n",
      "0    O2ewH  [[[228], [228], [228], [228], [228], [228], [2...   \n",
      "1    pPDId  [[[126], [126], [126], [126], [126], [126], [1...   \n",
      "2    Qwlss  [[[255], [255], [255], [255], [255], [255], [2...   \n",
      "3    7g77Y  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...   \n",
      "4    Eeh92  [[[255], [255], [255], [255], [255], [255], [2...   \n",
      "\n",
      "                                           binarized  \n",
      "0  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...  \n",
      "1  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...  \n",
      "2  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...  \n",
      "3  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...  \n",
      "4  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get list of image files and shuffle them\n",
    "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Limit the dataset to the first 100000 images\n",
    "image_files = image_files[:100000]\n",
    "\n",
    "# Create lists to store filenames, images, and their binarized versions\n",
    "filenames = []\n",
    "images = []\n",
    "binarized_images = []\n",
    "original_shapes = []\n",
    "\n",
    "# Loop through images and process them\n",
    "for idx, image_file in enumerate(image_files):\n",
    "    image_path = os.path.join(dataset_path, image_file)\n",
    "    filename = os.path.splitext(image_file)[0]\n",
    "\n",
    "    # Read the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Skip unreadable images\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # Binarize the image\n",
    "    binarized_image = binarize(image)\n",
    "\n",
    "    # Append to lists\n",
    "    filenames.append(filename)\n",
    "    images.append(image)\n",
    "    binarized_images.append(binarized_image)\n",
    "    original_shapes.append(image.shape)\n",
    "\n",
    "    if idx < 5:\n",
    "        print(f\"{filename} shape: {image.shape}\")\n",
    "\n",
    "# Check if all images have the same shape\n",
    "unique_shapes = set(original_shapes)\n",
    "if len(unique_shapes) == 1:\n",
    "    h, w = list(unique_shapes)[0]\n",
    "    print(f\"Image shape: {h} x {w}\")\n",
    "else:\n",
    "    print(\"Images have varying sizes. Consider resizing.\")\n",
    "    \n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "binarized_images = np.array(binarized_images)\n",
    "\n",
    "# Add a channel dimension (since we're using grayscale)\n",
    "images = images[..., np.newaxis]\n",
    "binarized_images = binarized_images[..., np.newaxis]\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'image': list(images),\n",
    "    'binarized': list(binarized_images)\n",
    "})\n",
    "\n",
    "# Preview DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36708da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11661d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "i = random.randint(0,49999)\n",
    "\n",
    "# Display image from the dataset\n",
    "rand_image = data.iloc[i]['binarized']  \n",
    "rand_image = rand_image.squeeze() \n",
    "\n",
    "\n",
    "plt.imshow(rand_image, cmap=\"gray\")\n",
    "plt.title(f\"{data.iloc[i]['filename']}\")\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078b33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dbd12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CHARSET and other related variables\n",
    "CHARSET = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "CHAR2IDX = {char: idx for idx, char in enumerate(CHARSET)}\n",
    "IDX2CHAR = {idx: char for char, idx in CHAR2IDX.items()}\n",
    "NUM_CLASSES = len(CHARSET)\n",
    "CAPTCHA_LEN = 5 \n",
    "\n",
    "def encode_label(label):\n",
    "    label = label[:CAPTCHA_LEN]  \n",
    "    encoded = [CHAR2IDX.get(c, 0) for c in label]  # Unknown chars map to index 0\n",
    "    return torch.tensor(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3be1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataset class to handle our images and labels\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx]['binarized']\n",
    "        label_str = self.data.iloc[idx]['filename']\n",
    "\n",
    "        # Ensure the image has 3 channels (ResNet expects 3 channels)\n",
    "        img = np.repeat(img, 3, axis=2)  # Convert to 3 channels if it's grayscale (repeating the grayscale channels)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = torch.tensor(img).permute(2, 0, 1)  # Convert to CHW format: [3, H, W]\n",
    "\n",
    "        label_tensor = encode_label(label_str)\n",
    "\n",
    "        return img, label_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0484df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets (80% training, 20% validation)\n",
    "train_df, val_df = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = CaptchaDataset(train_df)\n",
    "val_dataset = CaptchaDataset(val_df)\n",
    "\n",
    "# Create DataLoader for both training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495c6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff2a7a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyrus\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vyrus\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class CaptchaResNet50(nn.Module):\n",
    "    def __init__(self, num_chars=5, num_classes=62):  # num_classes = size of CHARSET\n",
    "        super(CaptchaResNet50, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet50\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the fully connected layer to match the number of classes (62 for each character)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes * num_chars)  # Output a flattened vector for 5 characters\n",
    "\n",
    "        # LSTM to process the sequence of characters\n",
    "        self.lstm = nn.LSTM(num_classes, 128, batch_first=True)  # Process each character\n",
    "        self.fc_out = nn.Linear(128, num_classes)  # Output layer to predict each character's class\n",
    "\n",
    "        self.num_chars = num_chars\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use ResNet50 to extract features from the image\n",
    "        x = self.resnet(x)  # [B, num_classes * num_chars]\n",
    "\n",
    "        # Reshape to sequence form (B, num_chars, num_classes) for LSTM\n",
    "        x = x.view(x.size(0), self.num_chars, self.num_classes)\n",
    "\n",
    "        # Apply LSTM to sequence of characters\n",
    "        x, _ = self.lstm(x)  # [B, num_chars, 128]\n",
    "\n",
    "        # Output layer to predict class for each character\n",
    "        x = self.fc_out(x)  # [B, num_chars, num_classes]\n",
    "\n",
    "        return x  # Shape: [B, num_chars, num_classes]\n",
    "\n",
    "# Example usage of the CaptchaResNet50 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the ResNet50-based model\n",
    "model = CaptchaResNet50().to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3be75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187b10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_wts = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_best_model(model)\n",
    "        elif val_loss < self.best_score - self.delta:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_best_model(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_best_model(self, model):\n",
    "        self.best_model_wts = model.state_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87c1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e06745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, loss function, etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CaptchaResNet50().to(device)  # Change to the ResNet-based model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8639350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc40623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m     26\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(outputs[:, i, :], labels[:, i])\n\u001b[1;32m---> 28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        # Compute the loss for each character\n",
    "        loss = 0\n",
    "        for i in range(labels.shape[1]):\n",
    "            loss += criterion(outputs[:, i, :], labels[:, i])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Learning rate: {param_group['lr']}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_imgs, val_labels in val_loader:\n",
    "            val_imgs = val_imgs.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_imgs)\n",
    "\n",
    "            val_loss_batch = 0\n",
    "            for i in range(val_labels.shape[1]):\n",
    "                val_loss_batch += criterion(val_outputs[:, i, :], val_labels[:, i])\n",
    "\n",
    "            val_loss += val_loss_batch.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Check early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    scheduler.step()  # Pass the validation loss to the scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da1b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "predictions = []\n",
    "labels_list = []\n",
    "image_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_imgs, test_labels in val_loader:  # Iterate over the test loader\n",
    "        test_imgs = test_imgs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        test_outputs = model(test_imgs)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(test_labels.shape[1]):\n",
    "            loss += criterion(test_outputs[:, i, :], test_labels[:, i])\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(test_outputs, 2)  # Get predicted class with the highest probability\n",
    "\n",
    "        correct += (predicted == test_labels).sum().item()\n",
    "        total += test_labels.size(0) * test_labels.size(1)\n",
    "\n",
    "        # Store predictions, true labels, and images for evaluation\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            for j in range(test_labels.shape[1]):\n",
    "                predictions.append(predicted[i, j].item())\n",
    "                labels_list.append(test_labels[i, j].item())\n",
    "                image_data.append(test_imgs[i].cpu().numpy())\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "test_loss_avg = test_loss / len(val_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss_avg:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "precision = precision_score(labels_list, predictions, average='macro', labels=np.unique(predictions))\n",
    "recall = recall_score(labels_list, predictions, average='macro', labels=np.unique(predictions))\n",
    "f1 = f1_score(labels_list, predictions, average='macro', labels=np.unique(predictions))\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
